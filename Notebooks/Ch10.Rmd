---
title: "ModernDive Ch10: Inference for Regression"
output: html_notebook
---

```{r}
# Load packages
library(tidyverse)
library(moderndive)
library(infer)

# Load function to clear libraries
source(here::here("clear_libraries.R"))
```

### 10.1 Regression refresher
#### 10.1.1 Teaching evaluations analysis

```{r}
evals_ch5 <- evals %>%
  select(ID, score, bty_avg, age)

evals_ch5 %>% 
glimpse()
```
```{r}
ggplot(evals_ch5, 
       aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", 
       y = "Teaching Score",
       title = "Relationship between teaching and beauty scores") +  
  geom_smooth(method = "lm", se = FALSE)
```
```{r}
# Fit regression model:
score_model <- lm(score ~ bty_avg, data = evals_ch5)
# Get regression table:
get_regression_table(score_model)
```

#### 10.1.2 Sampling scenario

Let’s view the instructors for these 463 courses as a representative sample from 
a greater study population. In our case, let’s assume that the study population 
is all instructors at UT Austin and that the sample of instructors who taught these 
463 courses is a representative sample. Unfortunately, we can only assume these 
two facts without more knowledge of the sampling methodology used by the researchers.

We can view our fitted slope b1 = 0.067 as a point estimate of the population slope β1. 
In other words, β1 quantifies the relationship between teaching score and “beauty” 
average bty_avg for all instructors at UT Austin. Similarly, we can view our fitted 
intercept b0 = 3.88 as a point estimate of the population intercept β0 for all 
instructors at UT Austin.

### 10.2 Interpreting regression tables

Given the lack of practical interpretation for the fitted intercept b0, in this 
section we’ll focus only on the second row of the table corresponding to the fitted 
slope b1. We’ll first interpret the std_error, statistic, p_value, lower_ci and 
upper_ci columns.

#### 10.2.1 Standard error

The standard error is the standard deviation of any point estimate computed from a sample.

#### 10.2.2 Test statistic

Here, our null hypothesis H0 assumes that the population slope β1 is 0. If the population
slope β1 is truly 0, then this is saying that there is no true relationship between teaching
and “beauty” scores for all the instructors in our population.

#### 10.2.3 p-value

A p-value is the probability of obtaining a test statistic just as extreme or more
extreme than the observed test statistic assuming the null hypothesis H0 is true.

Recall that you can intuitively think of the p-value as quantifying how “extreme” 
the observed fitted slope of b1 = 0.067 is in a  “hypothesized universe” where there 
is no relationship between teaching and “beauty” scores.

#### 10.2.4 Confidence interval

The statistically precise interpretation of this confidence interval is: “if we 
repeated this sampling procedure a large number of times, we expect about 95% of the 
resulting confidence intervals to capture the value of the population slope β1.” 
However, we’ll summarize this using our shorthand interpretation that 
“we’re 95% ‘confident’ that the true population slope β1 lies between 0.035 and 0.099.”

Since β1 = 0 is not in our plausible range of values for β1, we are inclined to 
believe that there, in fact, is a relationship between teaching and “beauty” scores 
and a positive one at that.

#### 10.2.5 How does R compute the table?

Much like the theory-based method for constructing confidence intervals you saw in 
Subsection 8.7.2 and the theory-based hypothesis test you saw in Subsection 9.6.1, 
there exist mathematical formulas that allow you to construct confidence intervals 
and conduct hypothesis tests for inference for regression.

### 10.3 Conditions for inference for regression
#### 10.3.1 Residuals refresher

```{r}
# Fit regression model:
score_model <- lm(score ~ bty_avg, data = evals_ch5)
# Get regression points:
regression_points <- get_regression_points(score_model)
regression_points
```
#### 10.3.2 Linearity of relationship

Would you say that the relationship between x and y is linear? It’s hard to say 
because of the scatter of the points about the line. In the authors’ opinions, we 
feel this relationship is “linear enough.”

#### 10.3.3 Independence of residuals

```{r}
evals %>% 
  select(ID, prof_ID, score, bty_avg)
```

In this case, we say there exists dependence between observations. The first four 
courses taught by professor 1 are dependent, the next 3 courses taught by professor 2 
are related, and so on. Any proper analysis of this data needs to take into account 
that we have repeated measures for the same profs.

So in this case, the independence condition is not met. What does this mean for 
our analysis?

#### 10.3.4 Normality of residuals
```{r}
ggplot(regression_points, aes(x = residual)) +
  geom_histogram(binwidth = 0.25, color = "white") +
  labs(x = "Residual")
```

This histogram shows that we have more positive residuals than negative.

Our regression model’s fitted teaching scores ˆy tend to underestimate the true 
teaching scores y. Furthermore, this histogram has a slight left-skew in that there 
is a tail on the left. This is another way to say the residuals exhibit a negative skew.

Is this a problem? Again, there is a certain amount of subjectivity in the response. 
In the authors’ opinion, while there is a slight skew to the residuals, we feel it 
isn’t drastic. On the other hand, others might disagree with our assessment.

#### 10.3.5 Equality of variance

```{r}
#  Plot of residuals over beauty score. 
ggplot(regression_points, aes(x = bty_avg, y = residual)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue", size = 1)
```

You can think of this figure as a modified version of the plot with the regression 
line in Figure 10.1, but with the regression line flattened out to y=0. Looking at 
this plot, would you say that the spread of the residuals around the line at y=0 is 
constant across all values of the explanatory variable x of “beauty” score? 

However, it can be argued that there isn’t a drastic non-constancy.

#### 10.3.6 What’s the conclusion?

```{r}
# (LC10.1) Continuing with our regression using age as the explanatory variable and teaching score as the outcome variable. 

# Use the get_regression_points() function to get the observed values, fitted values, and residuals for all 463 instructors. Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.

score_model2 <-
  evals_ch5 %>% 
  lm(score ~ age, data = .)

regression_points2 <-
  score_model2 %>% 
  get_regression_points()
```

```{r}
# L
evals_ch5 %>% 
  ggplot(aes(age, score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# A: Yes
```

```{r}
# N  
regression_points2 %>% 
  ggplot(aes(residual)) +
  geom_histogram()

# A: somewhat normal (with left skew)
```
```{r}
# E
regression_points2 %>% 
  ggplot(aes(age, residual)) +
  geom_point() +
  geom_hline(yintercept = 0, size = 1, color = "blue")

# A: Acceptable; negative skew visible
```

### 10.4 Simulation-based inference for regression
#### 10.4.1 Confidence interval for slope

```{r}
# Let’s first construct the bootstrap distribution for the fitted slope b1:
bootstrap_distn_slope <-
  evals %>% 
  specify(score ~ bty_avg) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "slope")
bootstrap_distn_slope
```

```{r}
# Let's visualize
bootstrap_distn_slope %>% visualize()
```

Percentile-method
```{r}
percentile_ci <-
  bootstrap_distn_slope %>% 
  get_ci(level = 0.95, type = "percentile")
percentile_ci
```

Standard error method
```{r}
obs_slope <-
  evals %>% 
  specify(score ~ bty_avg) %>% 
  calculate(stat = "slope")
obs_slope
```

```{r}
se_ci <-
  bootstrap_distn_slope %>% 
  get_ci(level = 0.95, type = "se", point_estimate = obs_slope)
se_ci
```

Comparing all three
```{r}
bootstrap_distn_slope %>% 
visualize() + 
  shade_confidence_interval(endpoints = percentile_ci, fill = NULL, 
                            linetype = "solid", color = "grey90") + 
  shade_confidence_interval(endpoints = se_ci, fill = NULL, 
                            linetype = "dashed", color = "grey60") +
  shade_confidence_interval(endpoints = c(0.035, 0.099), fill = NULL, 
                            linetype = "dotted", color = "black")
```

#### 10.4.2 Hypothesis test for slope

```{r}
null_distn_slope <- 
  evals %>% 
  specify(score ~ bty_avg) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "slope")
null_distn_slope
```

```{r}
# Visualize null distribution and p-value
null_distn_slope %>% 
  visualize() + 
  shade_p_value(direction = "both", obs_stat = obs_slope)
```

```{r}
# Get p-value
null_distn_slope %>% 
  get_p_value(obs_stat = obs_slope, direction = "both")
```

```{r}
# (LC10.2) Repeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of stat = "correlation" in the calculate() function of the infer package.

null_distn_cc <- 
  evals %>% 
  specify(score ~ bty_avg) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "correlation")

obs_cc <- 
  evals %>% 
  observe(score ~ bty_avg, stat = "correlation")

null_distn_cc %>% 
  visualize() + 
  shade_p_value(direction = "both", obs_stat = obs_cc)
```
```{r}
null_distn_cc %>% 
get_p_value(obs_stat = obs_cc, direction = "both")
```

```{r}
clear_libraries()
```
